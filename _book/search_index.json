[["index.html", "Preface", " Preface This textbook is designed to support progress in the Advanced Higher Statistics course by providing concise notes, example solutions and exercises to practice skills. It will be helpful to have the access to the following resources: Whilst the facts and skills contained within these pages are important, they should be enhanced by opportunities for discussion and practical activities to give the deeper insight that should be the goal for anyone taking the course. "],["probability-theory.html", "1 Probability theory", " 1 Probability theory The primary goal of a statistician is to make sense of the world by analysing data. Typically, however, they will only be able to obtain data on a sample from the full population of interest, which in order to avoid bias should be a random sample. For this reason, any course in statistics should begin with a study of probability theory to help understand and manage the uncertainty that arises from this. Key definitions In probability theory, an experiment for which all of the possible outcomes can be defined, and yet the actual outcome of any given trial cannot be known in advance, is called a random experiment. The set of all possible outcomes is called the sample space of the random experiment. Probability is assigned to individual outcomes or to sets of outcomes, called events. Probability itself is defined as the long-run relative frequency of an outcome or event occurring. For example, in the context of rolling a fair, six-sided die: the procedure of rolling the die and observed the result is a random experiment. each roll of the die may be referred to as a trial. the sample space consists of a list of all the the possible outcomes: \\(\\{1,2,3,4,5,6\\}\\) the probability of a \\(5\\) is \\(\\frac{1}{6}\\) since in the long-run it can be expected to occur \\(\\frac{1}{6}\\) of the time. a possible event of interest may be that the outcome is prime: \\(\\{2,3,5\\}\\) Notation The probability that a fair, cubical die lands on 5 is one-in-six, or \\(\\frac{1}{6}\\). Probability notation can be used to write this sentence far more efficiently as: \\[P(5)=\\frac{1}{6}\\] "],["theoretical-probability.html", "1.1 Theoretical Probability", " 1.1 Theoretical Probability When each of the outcomes in the sample space of a random experiment can be reasonably assumed to be equally likely then the probability of any outcomes or events can be determined in advance. This is called theoretical probability. Sample spaces and equally-likely outcomes Equally-likely outcomes typically arise from the symmetry of a physical object (such as a coin or die) or a reasonable assumption. For example, when considering the day a randomly chosen person was born on, it seems reasonable to assume any day of the week is equally likely. Since sample spaces contain every possible outcome, the probability that the outcome of a trial is contained in the sample space is 1, meaning it is certain. Hence, the probability of each outcome is \\(\\frac{1}{N}\\) where \\(N\\) is the number of outcomes. Events An event is typically a set of outcomes. Assuming the outcomes are equally likely, the probability of an event occurring can be calculated. Note that for any event: It can help to write out sample spaces. Some sample spaces can be easily described using a simple list, but more organisation may be needed for more complicated sample spaces, such as approaching the list in a structured manner or using a grid. Remember that sample spaces are seldom useful if each of the outcomes is not equally likely. If two coins are tossed: Not equally likely outcomes: \\(\\{\\text{no tails},\\text{one tails},\\text{two tails}\\}\\) Equally likely outcomes: \\(\\{\\text{HH},\\text{HT},\\text{TH},\\text{TT}\\}\\) Exercise 1A For each question, assume “at random” and “randomly” imply fairly.   "],["experimental-probability.html", "1.2 Experimental Probability", " 1.2 Experimental Probability When the outcomes of a random experiment are not equally likely, the probability of any outcome or event can’t be determined theoretically. In these instances the probability of an event can be estimated instead through repeated trials - this is referred to as obtaining the experimental probability of the event. For example, if a motorist passes a set of traffic lights every morning and wishes to know the probability of having to wait at a red light on any given morning they could calculate the proportion of times they have to wait at a red light. If, over the course of 200 mornings, they have to wait at a red light 114 times: \\[P(\\text{red light})\\approx\\frac{114}{200}=\\frac{57}{100}=0.57\\] The greater the number of trials used, the more likely it is that this proportion will accurately approximate the probability of the event. Generally, experimental probabilities should be considered less reliable than theoretical probabilities. However, many statisticians make valid use of experimental probabilities that have been obtained either by a sufficiently large number of observations or long-standing, historical knowledge of the probability of an event occuring. "],["tree-diagrams.html", "1.3 Tree Diagrams", " 1.3 Tree Diagrams Tree diagrams can be created to map out the possible results of a random experiment, and they are especially useful when considering two or more events. For example, suppose a game involves rolling a fair, cubical die hoping for it to land on a 6, and tossing a fair coin: Note: Every branch is clearly labeled with a probability. Each node is clearly described, such as “six”. Instead of not six you could instead write \\(\\overline{\\text{six}}\\). Sibling branches add to 1, as do all of the “and” probabilities. Multiplying along branches gives the “and” probabilities. Note that in the tree diagram above the probability of obtaining heads or tails on the toss of a coin does not change depending on whether the roll of a die gives a six or not. In the example on the next page the probabilities on the second stage of the tree diagram do change depending on the result of the first stage. Pages 14 and 15 discuss this in more detail. Exercise 1B   "],["complementary-events.html", "1.4 Complementary Events", " 1.4 Complementary Events Venn diagrams can be a useful way of visualising a sample space. Below can be seen which equally-likely outcomes of the roll of a die are square numbers, and which are not. The event \\(\\textbf{not A}\\), often notated as \\(\\overline{\\mathbf{A}}\\) or \\(\\mathbf{A&#39;}\\), is called the \\(\\textbf{complement of }\\mathbf{A}\\). "],["intersections-and-unions.html", "1.5 Intersections and Unions", " 1.5 Intersections and Unions Evaluating the value of the intersection of union or two or more events and comparing the results to the laws on the previous page can show whether they are mutually exclusive and/or exhaustive. It may also be helpful to look back at Pages 5 and 7 to see that intersections (and) as well as unions (or) have already been calculated without using the more technical language now introduced. Exercise 1C   "],["the-addition-rule.html", "1.6 The Addition Rule", " 1.6 The Addition Rule A rule for calculating the union of events can now be derived by considering its representation on a Venn diagram. The rule above is valid for all events \\(A\\),\\(B\\), but simpler verions can be used when certain conditions are satisfied. Recall that for mutually exclusive events, the probability of the intersection is 0 (impossible), and for exhaustive events the probability of the union is 1 (certain). To work with probability rules, it helps to carefully notate the information known from the question, paying attention to words such as exhaustive and mututally exclusive, then consider which rule may connect the known probabilities with the probabilities being asked for. Exercise 1D   "],["conditional-probability.html", "1.7 Conditional Probability", " 1.7 Conditional Probability Suppose 17 pupils in a class are asked whether they love Art, and whether they love Biology, and the results are displayed in both a Venn diagram and a contingency table. Note that the right hand column shows the total number of pupils that love Biology, and the total that don’t, whilst the final row similarly shows the total number of pupils that love Art, and the total that don’t. These are called marginal totals, and the marginal column and marginal row should both sum to the total number of observations, in this case 10 pupils. The marginal totals can be used to calculate the probability that a randomly chosen pupil loves Art, irrespective of whether they love Biology or not: \\(P(\\text{Art})=\\frac{5}{10}\\). If a pupil is randomly chosen and it is first discovered that they love Biology, then the probability they also love Art given that they love Biology is \\(\\frac{3}{7}\\). This is called a conditional probability and is notated as \\(P(\\text{Art | Biology})\\) or \\(P(\\text{Art given Biology})\\). The rule for conditional probabilities below is one of the most significant in the context of much of the statistical analysis to come in this course, and it should be carefully learned. Note that, in general, \\(\\mathbf{P(A|B)\\ne P(B|A)}\\) and care should be taken not to mix these up. It should be noted that \\(P(\\bar{A}|B)=1-P(A|B)\\), whilst in general \\(P(A|\\bar{B})\\ne 1-P(A|B)\\). Care should be taken to only make use of the valid complement to conditional probabilities. Exercise 1E   "],["independent-events.html", "1.8 Independent Events", " 1.8 Independent Events The probability of tossing a coin and it landing tails up is \\(\\frac{1}{2}\\). The probability of it landing tails up given that it is a weekday, notated \\(P(\\text{tails }|\\text{ weekday})\\) is still \\(\\frac{1}{2}\\). Since knowing that it is a weekday has no effect on the coin landing tails up, the two events are said to be independent of each other. In comparison, the probability that a randomly selected person has an umbrella with them is likely to be different than the probability they have an umbrella given that it is a rainy day. Since knowing whether it is rainy changes the probability of them having an umbrella it is said that the two events are not independent of each other. The conditional probability rule can be rearranged to allow the intersection of two events to be calculated using a conditional probability, and for independent events, since \\(P(B|A)=P(B)\\), this simplifies further. Exercise 1F   "],["tree-diagrams-and-probability-notation.html", "1.9 Tree Diagrams and Probability Notation", " 1.9 Tree Diagrams and Probability Notation On pages 6 and 7 tree diagrams were used to determine the probabilities of the intersections of events by multiplying along branches. Where events on the first stage of a diagram had no impact on the probabilities of events on the second stage this was because the events were independent. Where the probabilities on the second stage changes based on previous events, this was becuase the events were not independent. Concepts such as independent, or not independent and notation for conditional probabilities and intersections can now be related to tree diagrams, as shown in below. Also note that at each split the branches should represent mutually exclusive and exhaustive possibilities. Tree Diagram Notation for Events A and B Tree Diagram Notation for Independent Events A and B The law of total probability says that the probability of an event \\(A\\), when its conditional probabilities given mutually exclusive and exhaustive events \\(B_i\\) are known, along with the probabilities of each of \\(B_i\\), is given by: \\[P(A)=P(B_1)\\times P(A|B_1)+P(B_2)\\times P(A|B_2)+P(B_3)\\times P(A|B_3)+...\\] This can be expressed using Sigma Notation, using the Greek letter \\(\\Sigma\\) (“Sigma”), representing a sum. As an example, if there are 4 events upon which \\(A\\) is conditional (\\(B_1,B_2,B_3,B_4\\)) then the total probability could be expressed as: \\[P(A)=\\sum_{i=1}^{4}P(B_i)\\times P(A|B_i)\\] Sigma notation is encountered often when examining the underlying mathematics of statistics. However, when all possible intersections are known, as is typically the case with tree diagrams, the total probability of an event \\(A\\) can be seen more simply as the sum of all intersections in which \\(\\mathbf{A}\\) is true. Exercise 1G "],["reversing-the-condition.html", "1.10 Reversing the Condition", " 1.10 Reversing the Condition Medical tests used to diagnose a condition in a patient are rarely, if ever, 100% reliable. For a patient who either has or does not have the condition, the test will come back with the result either report positive or negative. For any positive result, the patient may either really have the condition (a true positive) or not actually have the condition (a false positive). For any negative result they may either really not have the condition (a true negative) or actually have the condition (a false negative). This can be summed up as: It is important for health professionals to understanding how to interpret tests results appropriately, with three key variables needed to do so correctly: Sensitivity - the % of patients with the condition correctly given a +ve result, or \\(P(\\text{positive | condition})\\). Specificity - the % of patients without the condition correctly given a -ve result, or \\(P(\\text{negative | }\\overline{\\text{condition}})\\). Prevalence - the proportion of individuals in the population that actually have the condition, or \\(P(\\text{condition})\\). If a patient tests positive for a condition the question will be relevant to them is: “Given that I tested positive, what is the probability that I actually do have the condition? Here, \\(P(\\text{positive | condition})\\) is known and the patient wishes to know \\(P(\\text{condition | positive})\\). This can be seen as a case of the more general problem of reversing the condition. In other words, if \\(P(A|B)\\) is known, how can \\(P(B|A)\\) be calculated? It may be surprising that a test which appears so effective can lead to such a high proportion of false positives, and calculations such as this should be considered when healthcare professionals establish testing procedures. In practice, the false positive rate can be drastically reduced by deciding when a test may be appropriate. GPs for example will ask questions about a patient’s symptoms and medical history to determine whether they have reason to suspect that the chance that they have the condition is significantly greater than a random member of the population, and therefore whether a test is appropriate. Exercise 1H "],["bayes-theorem.html", "1.11 Bayes Theorem", " 1.11 Bayes Theorem In 1763 the Reverend Thomas Bayes, also a statistician, published “An Essay towards solving a Problem in the Doctrine of Chances”. In it, he set out both a formula that has come to be known as Bayes’ Theorem as well as an approach to thinking about combining existing knowledge with new information that can be referred to as Bayesian thinking. In turn, this thinking lead to an approach towards statistcal inference that has been growing since the mid-20th century called Bayesian statistics, substantially made more feasible by modern computing power. Note: Whilst knowledge of Bayes’ Theorem is indicated as part of the Advanced Higher Statistics course specification, Bayesian statistics or even Bayesian thinking are not part of this course, and so will not be covered here. If you would like to find out what these are and whether they may be relevant to you in the future, it is recommended that you discuss this with your teacher at a later point in the course, after gaining a thorough understanding of hypothesis testing. Modules introducing these ideas are typically available in the second or third years of an undergraduate degree in Mathematics or Statistics. Bayes’ Theorem itself is easily derived by remembering that \\(P(A\\cap B)=P(B\\cap A)\\) and equating the multiplication rules for each, then rearranging to give: The reason for the shortness of this section is that any probability problem for which Bayes’ Theorem may be used can also be solved often more simply using a tree diagram and the rule for conditional probability. Nevertheless, you should be aware of the formula and it may be of use. Exercise 1I   "],["review-exercise.html", "Review Exercise", " Review Exercise "],["the-distribution-of-the-sample-mean.html", "2 The Distribution of the Sample Mean", " 2 The Distribution of the Sample Mean Chapters 5, 6 and 7 cover the probability of observing given values, or ranges of values, when the distribution of a random variable is known. However, statisticians are rarely in the business of making a single observation from a population - instead, they are far more likely to draw a sample of observations. Then, rather than considering each individually, the mean of the sample is typically calculated - this is called the sample mean. Given random variable \\(X\\), the sample mean of \\(X\\) is also a random variable and is denoted \\(\\bar{X}\\). It is calculated as: \\[\\bar{X}=\\frac{\\sum{X_i}}{n}=\\frac{X_1+X_2+X_3+...+X_n}{n}\\] "],["the-expectation-and-variance-of-the-sample-mean.html", "2.1 The Expectation and Variance of the Sample Mean", " 2.1 The Expectation and Variance of the Sample Mean The expected value of the sample mean is equal to the true value of the population mean, meaning it is an unbiased estimator of the population mean. That is, if a series of samples are taken from a distribution with population mean \\(\\mu\\) and the mean is calculated for each sample, in the long-run the mean of these sample means tends towar \\(\\mu\\). The standard deviation of the sample mean \\(\\bar{X}\\) is called the standard error, and is smaller than the standard deviation of the underlying distribution \\(X\\). For an underlying population with standard deviation \\(\\sigma\\), the standard error of the sample mean for a sample of size \\(n\\) is \\(\\frac{\\sigma}{n}\\). Proof: Given random variable \\(X\\) such that \\(E(X)=\\mu\\) and \\(V(X)=\\sigma^2\\): \\[\\begin{align*} E(\\bar{X})&amp;=E(\\frac{X_1+X_2+...+X_n}{n})\\\\ &amp;=E(\\frac{E(X_1)}{n}+\\frac{E(X_2)}{n}+...+\\frac{E(X_n)}{n}\\\\ &amp;=\\frac{1}{n}(E(X_1)+\\frac{1}{n}E(X_2)+...+\\frac{1}{n}E(X_n))\\\\ &amp;=\\frac{1}{n}(n\\mu)\\\\ &amp;=\\mu \\end{align*}\\]   \\[\\begin{align*} V(\\bar{X})&amp;=V(\\frac{X_1+X_2+...+X_n}{n})\\\\ &amp;=V(\\frac{V(X_1)}{n}+\\frac{V(X_2)}{n}+...+\\frac{V(X_n)}{n}\\\\ &amp;=\\frac{1}{n^2}(V(X_1)+\\frac{1}{n^2}V(X_2)+...+\\frac{1}{n^2}V(X_n))\\\\ &amp;=\\frac{1}{n^2}(n\\sigma^2)\\\\ &amp;=\\frac{\\sigma^2}{n} \\end{align*}\\] These results assume the observations are independent, which can be assured by using random sampling. "],["the-sample-mean-of-a-normally-distributed-random-variable.html", "2.2 The Sample Mean of a Normally Distributed Random Variable", " 2.2 The Sample Mean of a Normally Distributed Random Variable Since the sample mean \\(\\bar{X}\\) is a linear combination of independent observations from \\(X\\), if \\(X\\) is normally distributed then \\(\\bar{X}\\) is also normally distributed. Due to the lower variance of the sample mean, extreme results are less likely when taking the mean of a sample in comparison to a single observation. For example, suppose the weights of adult red squirrels in a location are known to be normally distributed, with mean 250 grams and standard deviation 12 grams. If a single squirrel is randomly selected and its weight measured, it would not be that unlikely to encounter a squirrel lighter than 240 grams or heavier than 260 grams. However, if a random sample of three squirrels is taken, each weighed and the mean of the sample calculated then it would be very unlikely for the sample mean to be lower than 240 grams or heavier than 260 grams. This is because that would likely involve randomly selecting not just one, but three particularly light (or particularly heavy) squirrels. The histograms below simulate 100 individual observations of squirrel weights compared to the means of 100 samples, each of size 3. Note that the sample means also show a distribution that looks normal with a mean of 250 grams, but that the spread of the sample means is much less than that of the individual observations. Whilst the \\(z\\)-transformation, used to calculate \\(z\\)-scores, for single observations from the distribution \\(X\\) uses the standard deviation in the denominator: \\[Z=\\frac{X-\\mu}{\\sigma}\\] The \\(z\\)-transformation for the sample mean, based on taking a random sample of observations of size \\(n\\) from \\(X\\), uses the standard error in the denominator instead: Exercise 2A "],["the-central-limit-theorem.html", "2.3 The Central Limit Theorem", " 2.3 The Central Limit Theorem The Central Limit Theorem (CLT) is one of the most important and useful concepts in statistics. It gives an insight into the prevalence of the normal distribution and, crucially for statisticians, allows the normal distribution to be used for statistical inference even when the data of interest is not normally distributed. The CLT says: Note that a “sufficiently large sample” in this course is greater than 20 (\\(n&gt;20\\)). Other statistical textbooks may use the value of 30, whilst in reality many statisticians appreciate that the nature of the underlying distribution may allow for smaller samples to be used or require larger samples in order for the distribution of the sample mean to be sufficiently approximately normal. Proof of the CLT is beyond the scope of this course (and this textbook), but statistical software and/or experimentation can help give a sense of what this looks like in practice. The simulations below show random individual observations from distributions on the left, with corresponding distributions of sample means to the right. Every random sample in both cases was of size 25. From this it can be seen that the Central Limit Theorem can be invoked for both discrete and continuous variables. The approximate distribution of the sample mean can be stated provided the expectation and variance (or standard deviation) of the underlying distribution are know or can be calculated. Remember that it is important to use the notation ‘\\(\\approx\\)’ to highlight that this is an approximate distribution: Exercise 2B "],["review-exercise-1.html", "Review Exercise", " Review Exercise "],["hypothesis-tests.html", "3 Hypothesis Tests", " 3 Hypothesis Tests Hypothesis testing is an objective approach to assessing the statistical evidence for a claim through the analysis of data from a random sample. The claim will typically relate to some aspect of the distribution of a population of interest, often a parameter such as the population mean, \\(\\mu\\). It is important to understand that the true value of this parameter cannot be known by studying a sample, so without a census of the entire population a statistician will never say that they have proved or disproved a claim or particular value. Instead, by studying the probability of obtaining the observed data, they will say they they have evidence to suggest or insufficient evidence to suggest a particular claim is true. "],["the-null-hypothesis-and-the-alternative-hypothesis.html", "3.1 The Null Hypothesis and the Alternative Hypothesis", " 3.1 The Null Hypothesis and the Alternative Hypothesis Most statistical studies are undertaken on the basis of a belief, a suspicion or a concern, which is called the alternative hypothesis, or \\(H_1\\). Initially in this chapter, these will consist of a claim about the value of the population mean, \\(\\mu\\). A statistician will initially adopt a conservative stance in opposition to the alternative hypothesis, assuming that claim is not true. This is called the null hypothesis, or \\(H_0\\). Exercise 9A For each proposed hypothesis test, state the null hypothesis and the alternative hypothesis using appropriate notation. "],["the-significance-level-and-the--value.html", "3.2 The Significance Level and the -value", " 3.2 The Significance Level and the -value One the null and alternative hypotheses have been stated, a conservative viewpoint is taken - it is assumed that the null hypothesis is true. Only observed data that is very unlikely to have occurred by random chance, if the null hypothesis were true, will lead to the decision to reject the null hypothesis in favour of the alternative hypothesis, at which point it can be said that there is evidence to suggest the alternative hypothesis is in fact the correct one. The probability of the such extreme data assuming the null hypothesis is true is called the p-value. A decision should be made in advance of collecting the data as to how small the \\(p\\)-value should be in order to decide to reject \\(H_0\\), and this chosen value is called the significance level, \\(\\alpha\\). When the \\(p\\)-value is less than the chosen level of significance, this is referred to as a statistically-significant result. Whilst 5%, or \\(\\alpha=0.05\\), is a commonly chosen level of significance, in some fields a smaller value is chosen to reduce the risk of making a Type I Error - incorrectly rejecting the null hypothesis when it is in fact true. At times, a higher significance level is chosen to help reduce the risk instead of a Type II Error - failing to reject the null hypothesis error when it is in fact false. When stating the conclusion to a hypothesis test, it is important to communicate it clearly referring to the context of the study as seen in the example on the following page, as well as referring to the level of significance used. "],["one-sample-z-test-for-the-population-mean.html", "3.3 One-Sample z-Test for the Population Mean", " 3.3 One-Sample z-Test for the Population Mean The one-sample \\(z\\)-test for the population mean is a hypothesis test used when a sample of data is taken from a population so that a claim about the true value of the population mean, \\(\\mu\\), can be assessed. It is important to know the conditions that must be satisfied for each type hypothesis test to be valid. If one or more of these conditions is not clearly known to be satisfied, then it may be valid to make an assumption that is is satisfied. In any statistical study these assumptions should be recognised and consideration should be given to how this assumption could be justified. If the sample size is greater than 20 (\\(n&gt;20\\)) then the first two conditions are not required. A sample size means that the CLT can be invoked, so the sample mean is normally-distributed regardless of the distribution of the underlying population. For sample sizes greater than 20 the sample standard deviation, \\(s\\), is considered to provide a sufficiently accurate estimate for \\(\\sigma\\). Since the \\(z\\)-test is assumes normally-distributed data, it is a parametric test - tests that do not assume the underlying data is normally-distributed are non-parametric tests. Once the conditions have been checked, necessary assumptions made and hypotheses stated, the \\(p\\)-value can be calculated and compared to the significance level. Exercise 9B "],["one-tailed-and-two-tailed-alternative-hypotheses.html", "3.4 One-Tailed and Two-Tailed Alternative Hypotheses", " 3.4 One-Tailed and Two-Tailed Alternative Hypotheses In each of the questions on Page ??, evidence was being sought for alternative hypotheses that suggested either the true value of the population mean is less than a specified value, or that it is greater than a specified value. When the alternative hypothesis is only interested in a single direction, the test being performed is described one-tailed. When instead the alternative hypothesis is interested in a change in either direction, it is referred to as a two-tailed test. If a coin is flipped 4 times and it comes up heads every time, the probability of such an extreme event can be calculated as \\(P(\\text{four heads})=(\\frac{1}{2})^4=\\frac{1}{16}=0.0625\\). However, an equally extreme event that would have also called into question the fairness of the coin would have been tails on every toss. Given the symmetry of the probability calculation, the actual \\(p\\)-value, the probability of such an extreme event, can be found by doubling the original, one-sided probability: \\[p\\text{-value }=2 \\times 0.0625 = 0.125\\] Since the normal distribution is also symmetrical, the \\(p\\)-value for two-tailed \\(z\\)-tests can also be calculated by doubling the equivalent one-tailed probability. Exercise 9C "],["test-statistics-and-critical-values.html", "3.5 Test Statistics and Critical Values", " 3.5 Test Statistics and Critical Values Every hypothesis test performed so far has involved calculating the probability of the observed sample mean, \\(\\bar{x}\\), by first using the \\(z\\)-transformation for a sample mean to obtain what is called a \\(z\\)-score. Instead of comparing a \\(p\\)-value to the significance level, another approach is to instead compare the \\(z\\)-score, referred to as the test statistic, to separately calculated cut-off values, called critical values. A \\(z\\)-test statistic of \\(0\\) occurs when the sample mean of the observed data is equal to the value for \\(\\mu\\) claimed by the null hypothesis. The further the test statistic is from \\(0\\), the more extreme the observed data is. One key need for this approach is that it can be difficult to calculate \\(p\\)-values for some distributions. Page 12 of the SQA Data Booklet can be used to find common critical values without the need for more extensive probability tables or a graphical calculator. It is expected that a statistician is able to both calculate and interpret a \\(p\\)-value when required, as well as use test statistics and critical values. For the remainder of this textbook, the predominant approach used in examples will be test statistics and critical values. Critical values can be found by considering where the 5% least-likely \\(z\\)-scores lie that, if observed, would cause the null hypothesis to be rejected. Note that for a two-tailed test, the 5% least-likely observed values for the sample mean are split between 2.5% in the upper tail of the distribution and 2.5% in the lower tail. Exercise 9D "],["one-sample--test-for-the-population-mean.html", "3.6 One-Sample -Test for the Population Mean", " 3.6 One-Sample -Test for the Population Mean In the early 20th century William Sealy Gosset, statistician and Head Experimental Brewer at Guinness, described the distribution of the sample mean when the population standard deviation is unknown. This was necessary since he was working with small sample sizes, for which the sample standard deviation could not be used as a reliable estimator for the population standard deviation. Since Guinness did not want rival companies to know that they were employing a statistician to improve their brewing, he published his work under the pen-name Student, hence the distribution he described becoming known as Student’s t-distribution. Not knowing the population standard deviation, \\(\\sigma\\), creates additional uncertainty about the distribution of the sample mean, with the sample standard deviation, \\(s\\), used instead. This results in the shape of a t-distribution resembling that of a normal distribution but with fatter tails. The \\(t\\)-distribution takes one parameter, degrees of freedom, or \\(\\nu\\), which is the number of data points from the sample that are free to vary under the null hypothesis. For the one-sample \\(t\\)-test for population mean the degrees of freedom can be calculated for a sample size \\(n\\) as: \\[\\nu=n-1\\] The one-sample \\(t\\)-test for population mean is a parametric test that should be used, in this course, in place of the equivalent \\(z\\)-test when the population standard deviation, \\(\\sigma\\), is not known and the sample size is less than 20. The \\(t\\)-test statistic is identical to that of the equivalent \\(z\\)-test, except the use of the sample standard deviation, \\(s\\), instead of the population standard deviation, \\(\\sigma\\). Critical values for the \\(t\\)-test can be found on Page 13 of the SQA Data Booklet, referring to the significance level, whether the test is one-tailed or two-tailed and the degrees of freedom. Exercise 9E "],["one-sample-z-test-for-the-population-proportion.html", "3.7 One-Sample z-Test for the Population Proportion", " 3.7 One-Sample z-Test for the Population Proportion If \\(X\\) of the elements from a random sample of size \\(n\\) from a population of interest satisfy some condition, then the sample proportion, \\(\\hat{p}\\), can be calculated as: \\[\\hat{p}=\\frac{X}{n}\\] For example, if a survey of registered voters reveals 114 out of 250 intend to vote for the incumbent candidate, the sample proportion, \\(\\hat{p}\\), is \\(\\frac{114}{250}=0.456\\). The true value of the population proportion, p, may be something different entirely. The one-sample \\(z\\)-test for proportion assesses whether an observed sample proportion provides statistically significant evidence against a null-hypothesised population proportion. The test statistic can be derived from the distributions of the random variables \\(X\\) and hence \\(\\hat{p}\\): \\[X \\sim B(n,p)\\] When \\(np&gt;5\\) and \\(nq&gt;5\\), \\(X\\) is approximately normally distributed: \\[\\begin{align*} E(X) &amp; = np\\\\ V(X) &amp; = npq\\\\ X &amp; \\approx N(np,npq) \\end{align*}\\] The distribution of \\(\\hat{p}\\) can now be obtained since \\(\\hat{p}=\\frac{X}{n}\\): \\[E(\\hat{p})=E(\\frac{X}{n})=\\frac{1}{n}E(X)=\\frac{1}{n}\\times np=p\\] \\[V(\\hat{p})=V(\\frac{X}{n})=(\\frac{1}{n})^2 V(X)=\\frac{1}{n^2}\\times npq=\\frac{pq}{n}\\] Therefore: \\[\\hat{p} \\sim N(p,\\frac{pq}{n})\\] Since the sample proportion is approximately normally distributed, the \\(z\\)-transformation leads to the required test statistic, valid when \\(\\mathbf{\\textbf{\\emph{np}}&gt;0}\\) and \\(\\mathbf{\\textbf{\\emph{nq}}&gt;0}\\): Exercise 9F "],["review-exercise-2.html", "Review Exercise", " Review Exercise "],["confidence-intervals.html", "4 Confidence Intervals", " 4 Confidence Intervals Many publications require statistical analyses to provide confidence intervals for parameters of interest, rather than only including point estimates (such as the sample mean) and results of hypothesis tests. For example: One key purpose of a confidence interval is to help give an idea of the possible “precision” of any point estimate that has been obtained. Typically, larger samples and lower variability in the data help to improve the “confidence” that we may have in any such interval. When statistical software is used on a computer to perform a hypothesis test, a confidence interval will often be reported in the output automatically: crisps&lt;-c(148,151,149,152,145,150,146,151) t.test(crisps,mu=150) ## ## One Sample t-test ## ## data: crisps ## t = -1.1282, df = 7, p-value = 0.2964 ## alternative hypothesis: true mean is not equal to 150 ## 95 percent confidence interval: ## 146.904 151.096 ## sample estimates: ## mean of x ## 149 "],["a-confidence-interval-for-the-population-mean.html", "4.1 A Confidence Interval for the Population Mean", " 4.1 A Confidence Interval for the Population Mean Exercise 10A "],["a-confidence-interval-for-the-population-proportion.html", "4.2 A Confidence Interval for the Population Proportion", " 4.2 A Confidence Interval for the Population Proportion Exercise 10B "],["review-exercise-3.html", "Review Exercise", " Review Exercise "],["answers.html", "Answers", " Answers Exercise 1A Exercise 1B   Exercise 1C   Three   Four "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
